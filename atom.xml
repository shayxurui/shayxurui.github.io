<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>xurui blog</title>
  
  <subtitle>blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-11-08T01:11:21.988Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>xu rui</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LAB</title>
    <link href="http://yoursite.com/2018/11/08/LAB/"/>
    <id>http://yoursite.com/2018/11/08/LAB/</id>
    <published>2018-11-08T01:09:44.000Z</published>
    <updated>2018-11-08T01:11:21.988Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="deep learning,face alignmnet" scheme="http://yoursite.com/tags/deep-learning-face-alignmnet/"/>
    
  </entry>
  
  <entry>
    <title>人体关键点综述</title>
    <link href="http://yoursite.com/2018/11/02/%E4%BA%BA%E4%BD%93%E5%85%B3%E9%94%AE%E7%82%B9%E7%BB%BC%E8%BF%B0/"/>
    <id>http://yoursite.com/2018/11/02/人体关键点综述/</id>
    <published>2018-11-02T07:11:59.000Z</published>
    <updated>2018-11-05T08:08:00.097Z</updated>
    
    <content type="html"><![CDATA[<p>人体关键点对于描述人体姿态，预测人体行为至关重要。因此人体关键点检测是诸多计算机视觉任务的基础，例如动作分类，异常行为检测，以及自动驾驶等等。近年来，随着深度学习技术的发展，人体关键点检测效果不断提升，已经开始广泛应用于计算机视觉的相关领域。本文主要介绍2D人体骨骼关键点的基本概念和相关算法，其中算法部分主要介绍基于深度学习的人体关键点检测。</p><a id="more"></a> <h1 id="相关数据集"><a href="#相关数据集" class="headerlink" title="相关数据集"></a>相关数据集</h1><h2 id="单人人体关键点数据集"><a href="#单人人体关键点数据集" class="headerlink" title="单人人体关键点数据集"></a>单人人体关键点数据集</h2><ol><li>LSP（Leeds Sports Pose Dataset）：单人人体关键点检测数据集，关键点个数为14，样本数2K，在目前的研究中基本上被弃用。<a href="http://sam.johnson.io/research/lsp.html" target="_blank" rel="noopener">LSP</a></li><li>FLIC（Frames Labeled In Cinema）：单人人体关键点检测数据集，关键点个数为9，样本数2W，在目前的研究中基本上被弃用。<a href="https://bensapp.github.io/flic-dataset.html" target="_blank" rel="noopener">FLIC</a></li></ol><h2 id="多人人体关键点数据集"><a href="#多人人体关键点数据集" class="headerlink" title="多人人体关键点数据集"></a>多人人体关键点数据集</h2><ol><li>MPII（MPII Human Pose Dataset）：单人/多人人体关键点检测数据集，关键点个数为16，样本数25K。<a href="http://human-pose.mpi-inf.mpg.de/" target="_blank" rel="noopener">MPII</a></li><li>MSCOCO：多人人体关键点检测数据集，关键点个数为17，样本数多于30W，目前的相关研究基本上还需要在该数据集上进行验证。<a href="http://cocodataset.org/#home" target="_blank" rel="noopener">MSCOCO</a></li><li>AI Challenger：多人人体关键点检测数据集，关键点个数为14，样本数约38W，竞赛数据集。<a href="https://challenger.ai/dataset/keypoint" target="_blank" rel="noopener">AI Challenger</a></li><li>PoseTrack：最新的关于人体骨骼关键点的数据集，多人人体关键点跟踪数据集，包含单帧关键点检测、多帧关键点检测、多人关键点跟踪三个人物，多于500个视频序列，帧数超过20K，关键点个数为15。<a href="https://posetrack.net/" target="_blank" rel="noopener">PoseTrack</a></li></ol><hr><p><center>人体骨骼关键点检测算法</center></p><h3 id="ground-truth构建方案"><a href="#ground-truth构建方案" class="headerlink" title="ground-truth构建方案"></a>ground-truth构建方案</h3><ol><li>直接回归坐标(Coordinate):Coordinate即直接将关键点坐标作为最后网络需要回归的目标，这种情况下可以直接得到每个坐标点的直接位置信息。首先将深度学习方法应用到人体关键点检测领域的网络(DeepPose)就是直接回归坐标。Coordinate网络在本质上来说，需要回归的是每个关键点的一个相对于图片的offset，而长距离offset在实际学习过程中是很难回归的，误差较大，同时在训练中的过程，提供的监督信息较少，整个网络的收敛速度较慢。</li><li>回归热点图(Heatmap):Heatmap即将每一类坐标用一个概率图来表示，对图片中的每个像素位置都给一个概率，表示该点属于对应类别关键点的概率，比较自然的是，距离关键点位置越近的像素点的概率越接近1，距离关键点越远的像素点的概率越接近0，具体可以通过相应函数进行模拟，如Gaussian等。回归热点图可以使用全卷积网络，大大减少了网络的参数量，</li><li>Heatmap+Offsets:与单纯的Heatmap不同的是，Google的Heatmap指的是在距离目标关键点一定范围内的所有点的概率值都为1，在Heatmap之外，使用Offsets，即偏移量来表示距离目标关键点一定范围内的像素位置与目标关键点之间的关系.为Heatmap + Offsets不仅构建了与目标关键点之间的位置关系，同时Offsets也表示了对应像素位置与目标关键点之间的方向信息，应该要优于单纯的Heatmap构建思路。</li></ol><h3 id="单人人体关键点检测算法"><a href="#单人人体关键点检测算法" class="headerlink" title="单人人体关键点检测算法"></a>单人人体关键点检测算法</h3><ol><li>DeepPose:Human Pose Estimation via Deep Neural Networks。这是使用神经网络处理人体关键点检测的第一篇文章。作者使用基于AlexNetwork的级联DNN来直接回归关键点的坐标。<a href="https://arxiv.org/abs/1312.4659" target="_blank" rel="noopener">DeepPose</a> <img src="/images/articalImage/deeppose.png" alt="网络结构"></li><li>Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation。首个回归heatmap。<a href="https://arxiv.org/abs/1406.2984" target="_blank" rel="noopener">paper</a></li></ol><h3 id="多人人体关键点检测算法"><a href="#多人人体关键点检测算法" class="headerlink" title="多人人体关键点检测算法"></a>多人人体关键点检测算法</h3><p>多人人体关键点检测主要分为两个方向：1. 自上而下(Top-Down)；2. 自下而上(Bottom-Up)。</p><ol><li><p>其中自上而下(Top-Down)的人体关键点检测算法主要包含两个部分，人体检测和单人人体关键点检测，即首先通过目标检测网络将每一个人都检测出来，然后在检测框的基础上对所有单人做人体关键点检测，其中代表性算法有G-RMI, CFN, RMPE, Mask R-CNN, and CPN，目前在MSCOCO数据集上最好的效果是72.6%。优点是网络简单，不涉及到复杂的数学操作，准确率较高，但是检测速度受行人的数量影响较大。</p><p> Hourglass Networks。由多个沙漏堆叠组成的网络，，通过重复进行bottom-up, top-down推断以估计人体姿态。沙漏网络可以捕捉不同尺度下图片所包含的信息。低分辨率的图片有较大的感受野，可以得到图片的全局信息，而低分辨率的图片有较小的感受野，可以得到图片的局部信息。沙漏首先通过conv层和pool层将特征缩小到很小的分辨率，再通过upsample将特征放大到较大的分辨率。重复进行该操作，每个沙漏模块都要加个loss进行监督，最终输出预测的heatmap。<a href="https://arxiv.org/abs/1603.06937" target="_blank" rel="noopener">Hourglass</a><img src="/images/articalImage/hourglass.png" alt="网络结构"></p><p> Cascaded Pyramid Network。这篇论文主要关注的是不同类别关键点的检测难度是不一样的，整个结构的思路是先检测比较简单的关键点、然后检测难的或不可见的关键点。网络分成两个stage:1.GlobalNet;2,RefineNet。GlobalNet主要负责检测容易检测的关键点，RefineNet主要解决难的或者不可见关键点的检测。在训练时取损失较大的top-K个关键点计算损失，然后进行梯度更新，不考虑损失较小的关键点。<a href="https://arxiv.org/abs/1711.07319" target="_blank" rel="noopener">Pyramid</a><img src="/images/articalImage/CPN.png" alt="网络结构"></p></li><li><p>自下而上(Bottom-Up)的方法同样包含两个部分，关键点检测和关键点聚类，即首先需要将图片中所有的关键点都检测出来，然后通过相关策略将所有的关键点聚类成不同的个体，其中对关键点之间关系进行建模的代表性算法有PAF, Associative Embedding, Part Segmentation, Mid-Range offsets，目前在MSCOCO数据集上最好的效果是68点是速度较快，不受行人数量影响，但是准确率较低，涉及到复杂的数学操作(聚类)。</p><p> Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields。Part Affinity Fields：该方法通过对人体的不同肢体结构进行建模，使用向量场来模拟不同肢体结构，解决了单纯使用中间点是否在肢干上造成的错连问题。<a href="https://arxiv.org/abs/1611.08050" target="_blank" rel="noopener">paper</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;人体关键点对于描述人体姿态，预测人体行为至关重要。因此人体关键点检测是诸多计算机视觉任务的基础，例如动作分类，异常行为检测，以及自动驾驶等等。近年来，随着深度学习技术的发展，人体关键点检测效果不断提升，已经开始广泛应用于计算机视觉的相关领域。本文主要介绍2D人体骨骼关键点的基本概念和相关算法，其中算法部分主要介绍基于深度学习的人体关键点检测。&lt;/p&gt;
    
    </summary>
    
    
      <category term="human pose estimation" scheme="http://yoursite.com/tags/human-pose-estimation/"/>
    
  </entry>
  
  <entry>
    <title>人脸关键点基本操作函数</title>
    <link href="http://yoursite.com/2018/10/30/FaceAlignment/"/>
    <id>http://yoursite.com/2018/10/30/FaceAlignment/</id>
    <published>2018-10-30T12:08:28.536Z</published>
    <updated>2018-10-30T12:08:28.492Z</updated>
    
    <content type="html"><![CDATA[<p>总结关于人脸数据预处理的函数。<br><a id="more"></a> </p><p>##读取图片和关键点:<br>    image=PIL.Image.open(image_path)<br>    landmark=np.loadtxt(pts_path,skiprows=3,comments=’}’)</p><pre><code>#或者landmarks = np.genfromtxt(pts_path, skip_header=3, skip_footer=1)</code></pre><p>##根据关键点得到边界框:<br>    minx = int(landmarks[:, 0].min())<br>    miny = int(landmarks[:, 1].min())<br>    maxx = int(landmarks[:, 0].max())<br>    maxy = int(landmarks[:, 1].max())    </p><p>##随机裁剪图片:<br>    h, w = image.shape[:2]<br>    new_h, new_w = output_size</p><pre><code>top = np.random.randint(0, h - new_h)left = np.random.randint(0, w - new_w)image = image[top: top + new_h,left: left + new_w]landmarks = landmarks - [left, top]</code></pre><p>##提取人脸:<br>    image = image.crop([bbox[0] - pad, bbox[1] - pad, bbox[2] + pad, bbox[3] + pad])<br>    landmarks = landmarks - (bbox[0] - pad, bbox[1] - pad)</p><blockquote><p>bbox是边界框坐标,bbox=(minx,miny,maxx,maxy).<br>pad是多裁剪部分,关键点减去裁剪后的起始坐标.</p></blockquote><p>##得到旋转矩阵:<br>    theta = np.radians(rotation_angle)<br>    c, s = np.cos(theta), np.sin(theta)<br>    mat = np.matrix(‘{} {} 0; {} {} 0’.format(c, -s, s, c), np.float32)</p><blockquote><p>mat=((cos,-sin,0),(sin,cos,0))</p></blockquote><p>##随机旋转图片:<br>    w, h = image.size<br>    image_rot = image.rotate(rotation_angle)<br>    w_rot, h_rot = image_rot.size<br>    center=(w/2,h/2)<br>    center_rot=(w_rot/2,h_rot/2)<br>    landmarks_rot=landmarks-center<br>    landmarks_rot = np.asarray(np.dot(landmarks_rot, manual_theta_inv)[:, :2])<br>    landmarks_rot=landmarks_rot+center_rot</p><blockquote><p>manual_theta_inv是旋转矩阵.</p></blockquote><p>##图片放缩:<br>    w, h = image.size<br>    image =torchvision.transforms.Resize((self.output_size, self.output_size))(image)<br>    landmarks = landmarks * [self.output_size / w, self.output_size / h]</p><p>##图片归一化:<br>    self.mean_img=np.mean(self.imgs,axis=(-3,-2))<br>    self.std_dev_img=np.std(self.imgs,axis=(-3,-2))<br>    self.imgs=(self.imgs-self.mean_img)/self.std_dev_img</p><p>##图片转换成张量:<br>    image=torchvision.transforms.ToTensor(image)<br>    landmarks=torch.from_numpy(landmarks).float().div(img_size)</p><p>##根据一个关键点，生成一个热点图:<br>    return np.zeros((heatmap.size, heatmap.size))</p><blockquote><p>如果关键点不可见，则返回全0矩阵</p></blockquote><pre><code>x_range = [i for i in range(heatmap.size)]y_range = [i for i in range(heatmap.size)]xx, yy = np.meshgrid(x_range, y_range)d2 = (xx - keypoint[0]) ** 2 + (yy - keypoint[1]) ** 2</code></pre><blockquote><p>xx是x_range的行扩展，yy是y_range的列扩展。xx-keypoint[0]，xx中第keypoint[0]列为0。yy-keypoint[1]，yy中第keypoint[1]行为0。d2,只有keypoint坐标下的值为0。</p></blockquote><pre><code>exponent = d2 / 2.0 / sigma / sigmaheatmap = np.exp(-exponent)</code></pre><blockquote><p>heatmap中只有关键点坐标值为1，其他坐标值皆小于1。</p></blockquote><p>##根据关键点生成热点图:<br>    landmarks = landmarks * [self.heatmap_size / image.size, self.heatmap_size / image.size]<br>    for i in range(68):<br>        flag = ~np.isnan(landmarks[i, 0])<br>        heatmap=…<br>        heatmap = heatmap[np.newaxis, …]<br>        hearmaps.append(heatmap)</p><blockquote><p>flag:该关键点是否被遮挡，或者不可见。如果不可见，则landmark值为NaN。</p></blockquote><p>##测试误差</p><blockquote><p>瞳孔距离:</p></blockquote><pre><code>normDist = np.linalg.norm(np.mean(gtLandmarks[36:42], axis=0) - np.mean(gtLandmarks[42:48], axis=0))</code></pre><blockquote><p>眼角距离:</p></blockquote><pre><code>normDist = np.linalg.norm(gtLandmarks[36] - gtLandmarks[45])</code></pre><blockquote><p>边界框对角线距离:</p></blockquote><pre><code>height, width = np.max(gtLandmarks, axis=0) - np.min(gtLandmarks, axis=0)normDist = np.sqrt(width ** 2 + height ** 2)</code></pre><blockquote><p>FAN的边界框对角线距离:</p></blockquote><pre><code>height, width = np.max(gtLandmarks, axis=0) - np.min(gtLandmarks, axis=0)normDist=np.sqrt(width*height)</code></pre><blockquote><p>测试误差:</p></blockquote><pre><code>err = np.mean(np.sqrt(np.sum((gtLandmarks - ptLandmarks) ** 2, axis=1))) / normDist</code></pre><p>##左右翻转图片:</p><blockquote><p>图片左右翻转:</p></blockquote><pre><code>img=np.array(np.fliplr(img))</code></pre><blockquote><p>关键点左右翻转:</p></blockquote><pre><code>landmark[:,:,0]=img.size[1]-landmark[:,:,0]    #把关键点的坐标置以图片的x坐标中点对称</code></pre><p>##扰动:</p><blockquote><p>从正态分布随机产生一个弧度，均值为0，方差是给定的弧度</p></blockquote><pre><code>angel=np.random.normal(0,rotation_std_dev_radian)</code></pre><blockquote><p>正太分布的标准差，均值为0，随机产生偏移</p></blockquote><pre><code>offset=[np.random.normal(0,translation_std_dev_x),np.random.normal(0,translation_std_dev_y)]</code></pre><blockquote><p>随机产生放缩比例</p></blockquote><pre><code>scaling=np.random.normal(1,scale_std_dev)</code></pre><blockquote><p>旋转矩阵:</p></blockquote><pre><code>R=np.array([[np.cos(angel),-np.sin(angel)],[np.sin(angel),np.cos(angel)]])</code></pre><p>##显示68个关键点在人脸中的位置:<br>    def drawLineChart(frame,i,j):</p><pre><code>    for k in range(i,j-1):        cv2.circle(frame,(preds[k,0],preds[k,1]),2,(255,255,255),-1)        cv2.line(frame,(preds[k,0],preds[k,1]),(preds[k+1,0],preds[k+1,1]),(255,255,255),1)    cv2.circle(frame,(preds[j-1,0],preds[j-1,1]),2,(255,255,255),-1)drawLineChart(frame,0,17)drawLineChart(frame,17,22)drawLineChart(frame,22,27)drawLineChart(frame,27,31)drawLineChart(frame,31,36)drawLineChart(frame,36,42)cv2.line(frame,(preds[41,0],preds[41,1]),(preds[36,0],preds[36,1]),(255,255,255),1)drawLineChart(frame,42,48)cv2.line(frame,(preds[47,0],preds[47,1]),(preds[42,0],preds[42,1]),(255,255,255),1)drawLineChart(frame,48,60)cv2.line(frame,(preds[59,0],preds[59,1]),(preds[48,0],preds[48,1]),(255,255,255),1)drawLineChart(frame,60,68)cv2.line(frame,(preds[67,0],preds[67,1]),(preds[60,0],preds[60,1]),(255,255,255),1)</code></pre><p>##高斯图:</p><p>###高斯核:<br>    for i in range(height):<br>            for j in range(width):<br>                gauss[i][j] = amplitude <em> math.exp(-(math.pow((j + 1 - center_x) / (<br>                    sigma_horz </em> width), 2) / 2.0 + math.pow((i + 1 - center_y) / (sigma_vert * height), 2) / 2.0))</p><p>###根据高斯核，生成高斯图:<br>    image=np.zeros((256,256))<br>    point=landmark<br>    sigma=2</p><pre><code>ul = [math.floor(point[0] - 3 * sigma), math.floor(point[1] - 3 * sigma)]     #ul=[xmin-6,ymin-6]br = [math.floor(point[0] + 3 * sigma), math.floor(point[1] + 3 * sigma)]     #br=[xmax-6,ymax+6]if (ul[0] &gt; image.shape[1] or ul[1] &gt;        image.shape[0] or br[0] &lt; 1 or br[1] &lt; 1):    return imagesize = 6 * sigma + 1g = _gaussian(size)  #高斯核  13*13g_x = [int(max(1, -ul[0])), int(min(br[0], image.shape[1])) -int(max(1, ul[0])) + int(max(1, -ul[0]))]g_y = [int(max(1, -ul[1])), int(min(br[1], image.shape[0])) -int(max(1, ul[1])) + int(max(1, -ul[1]))]img_x = [int(max(1, ul[0])), int(min(br[0], image.shape[1]))]img_y = [int(max(1, ul[1])), int(min(br[1], image.shape[0]))]assert (g_x[0] &gt; 0 and g_y[1] &gt; 0)image[img_y[0] - 1:img_y[1], img_x[0] - 1:img_x[1]] = image[img_y[0] - 1:img_y[1], img_x[0] - 1:img_x[1]] + g[g_y[0] - 1:g_y[1], g_x[0] - 1:g_x[1]]#image[ymin-1:ymax,xmin-1:xmax]=image[ymin-1:ymax,xmin-1:xmax]+g[1-1:y_max-ymin+1,1-1:x_max-xmin+1]image[image &gt; 1] = 1return image</code></pre><p>##从热点图中得到关键点坐标：<br>     max, idx = torch.max(                                                                            #max:(1,68)         idx:(1,68)<br>        hm.view(hm.size(0), hm.size(1), hm.size(2) * hm.size(3)), 2)  #(1,68,65536)<br>    idx += 1<br>    preds = idx.view(idx.size(0), idx.size(1), 1).repeat(1, 1, 2).float()        #preds:(1,68,2)<br>    preds[…, 0].apply_(lambda x: (x - 1) % hm.size(3) + 1)          #  x坐标. x=idx%hm.size(3)<br>    preds[…, 1].add_(-1).div_(hm.size(2)).floor_().add_(1)          # y坐标. y=idx/hm.size(2)<br>    for i in range(preds.size(0)):         #1<br>        for j in range(preds.size(1)):         #68<br>            hm_ = hm[i, j, :]        #(256,256)<br>            pX, pY = int(preds[i, j, 0]) - 1, int(preds[i, j, 1]) - 1<br>            if pX &gt; 0 and pX &lt; 63 and pY &gt; 0 and pY &lt; 63:<br>                diff = torch.FloatTensor(<br>                    [hm_[pY, pX + 1] - hm_[pY, pX - 1],<br>                     hm_[pY + 1, pX] - hm_[pY - 1, pX]])</p><pre><code>            preds[i, j].add_(diff.sign_().mul_(.25))preds.add_(-.5)preds_orig = torch.zeros(preds.size())                                      #preds_orig:(1,68,2)if center is not None and scale is not None:    for i in range(hm.size(0)):        for j in range(hm.size(1)):            preds_orig[i, j] = transform(                preds[i, j], center, scale, hm.size(2), True)return preds, preds_orig</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;总结关于人脸数据预处理的函数。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>maskrcnn阅读笔记</title>
    <link href="http://yoursite.com/2018/10/30/maskrcnn%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/10/30/maskrcnn阅读笔记/</id>
    <published>2018-10-30T11:20:42.000Z</published>
    <updated>2018-11-01T08:54:19.441Z</updated>
    
    <content type="html"><![CDATA[<p>这是我阅读Facebook发布基于pytorch1.0版本的mask r cnn所做的笔记。<br><a id="more"></a> </p><p>#优化器</p><pre><code>optimizer = make_optimizer(cfg, model)                 #得到学习率为0.02,权值衰减为0.0001，冲量是0.9的SGD优化器  </code></pre><hr><pre><code>def make_optimizer(cfg, model):    params = []    for key, value in model.named_parameters():        if not value.requires_grad:                      #若值不需要梯度计算，则跳过这次循环            continue        lr = cfg.SOLVER.BASE_LR                 # 0.02              weight_decay = cfg.SOLVER.WEIGHT_DECAY            # 0.0001        if &quot;bias&quot; in key:            lr = cfg.SOLVER.BASE_LR * cfg.SOLVER.BIAS_LR_FACTOR            weight_decay = cfg.SOLVER.WEIGHT_DECAY_BIAS        # 0.0001        params += [{&quot;params&quot;: [value], &quot;lr&quot;: lr, &quot;weight_decay&quot;: weight_decay}]    optimizer = torch.optim.SGD(params, lr, momentum=cfg.SOLVER.MOMENTUM)        #返回SGD优化器    return optimizer</code></pre><p>#学习率更新规则</p><pre><code>scheduler = make_lr_scheduler(cfg, optimizer)</code></pre><hr><pre><code>def make_lr_scheduler(cfg, optimizer):    return WarmupMultiStepLR(        optimizer,        cfg.SOLVER.STEPS,        cfg.SOLVER.GAMMA,        warmup_factor=cfg.SOLVER.WARMUP_FACTOR,        warmup_iters=cfg.SOLVER.WARMUP_ITERS,        warmup_method=cfg.SOLVER.WARMUP_METHOD,    )class WarmupMultiStepLR(torch.optim.lr_scheduler._LRScheduler):    def __init__(        self,        optimizer,             #SGD        milestones,                # (60000, 80000)           gamma=0.1,       # 0.1        warmup_factor=1.0 / 3,              #  1.0/3        warmup_iters=500,        # 500        warmup_method=&quot;linear&quot;,   # &quot;linear&quot;        last_epoch=-1,    ):        if not list(milestones) == sorted(milestones):            raise ValueError(                &quot;Milestones should be a list of&quot; &quot; increasing integers. Got {}&quot;,                milestones,            )        if warmup_method not in (&quot;constant&quot;, &quot;linear&quot;):            raise ValueError(                &quot;Only &apos;constant&apos; or &apos;linear&apos; warmup_method accepted&quot;                &quot;got {}&quot;.format(warmup_method)            )        self.milestones = milestones        self.gamma = gamma        self.warmup_factor = warmup_factor        self.warmup_iters = warmup_iters        self.warmup_method = warmup_method        super(WarmupMultiStepLR, self).__init__(optimizer, last_epoch)    def get_lr(self):        warmup_factor = 1        if self.last_epoch &lt; self.warmup_iters:             if self.warmup_method == &quot;constant&quot;:                warmup_factor = self.warmup_factor            elif self.warmup_method == &quot;linear&quot;:                alpha = self.last_epoch / self.warmup_iters          # -0.002                warmup_factor = self.warmup_factor * (1 - alpha) + alpha     # 500.98        return [            base_lr* warmup_factor* self.gamma ** bisect_right(self.milestones, self.last_epoch)            for base_lr in self.base_lrs        ]</code></pre><p>#从checkpoint载入预模型</p><pre><code>checkpointer = DetectronCheckpointer(cfg, model, optimizer, scheduler, output_dir, save_to_disk)extra_checkpoint_data = checkpointer.load(cfg.MODEL.WEIGHT)  </code></pre><hr><pre><code>def load(self, f=None):             #f=&quot;catalog://ImageNetPretrained/MSRA/R-50&quot;   if self.has_checkpoint():            #判断checkpoint文件是否存在       # override argument with existing checkpoint       f = self.get_checkpoint_file()        #读取checkpoint文件   if not f:                                     #如果checkpoint不存在，则从头开始初始化模型，并返回{}       # no checkpoint could be found       self.logger.info(&quot;No checkpoint found. Initializing model from scratch&quot;)       return {}   self.logger.info(&quot;Loading checkpoint from {}&quot;.format(f))   checkpoint = self._load_file(f)         self._load_model(checkpoint)                 #载入模型   if &quot;optimizer&quot; in checkpoint and self.optimizer:         #读取优化器       self.logger.info(&quot;Loading optimizer from {}&quot;.format(f))       self.optimizer.load_state_dict(checkpoint.pop(&quot;optimizer&quot;))   if &quot;scheduler&quot; in checkpoint and self.scheduler:            #读取学习率更新规则       self.logger.info(&quot;Loading scheduler from {}&quot;.format(f))       self.scheduler.load_state_dict(checkpoint.pop(&quot;scheduler&quot;))   # return any further checkpoint data   return checkpoint</code></pre><p><span id="1"></span></p><p>#载入数据</p><pre><code>data_loader = make_data_loader(cfg,is_train=True,is_distributed=distributed,start_iter=arguments[&quot;iteration&quot;],)</code></pre><hr><pre><code>def make_data_loader(cfg, is_train=True, is_distributed=False, start_iter=0):    num_gpus = get_world_size()      # 1    if is_train:        images_per_batch = cfg.SOLVER.IMS_PER_BATCH         # 16        assert (            images_per_batch % num_gpus == 0        ), &quot;SOLVER.IMS_PER_BATCH ({}) must be divisible by the number &quot;        &quot;of GPUs ({}) used.&quot;.format(images_per_batch, num_gpus)        images_per_gpu = images_per_batch // num_gpus               # 16        shuffle = True        num_iters = cfg.SOLVER.MAX_ITER                 # 90000    else:        images_per_batch = cfg.TEST.IMS_PER_BATCH            # 8        assert (            images_per_batch % num_gpus == 0        ), &quot;TEST.IMS_PER_BATCH ({}) must be divisible by the number &quot;        &quot;of GPUs ({}) used.&quot;.format(images_per_batch, num_gpus)        images_per_gpu = images_per_batch // num_gpus          # 8        shuffle = False if not is_distributed else True        num_iters = None        start_iter = 0    if images_per_gpu &gt; 1:        logger = logging.getLogger(__name__)        logger.warning(            &quot;When using more than one image per GPU you may encounter &quot;            &quot;an out-of-memory (OOM) error if your GPU does not have &quot;            &quot;sufficient memory. If this happens, you can reduce &quot;            &quot;SOLVER.IMS_PER_BATCH (for training) or &quot;            &quot;TEST.IMS_PER_BATCH (for inference). For training, you must &quot;            &quot;also adjust the learning rate and schedule length according &quot;            &quot;to the linear scaling rule. See for example: &quot;            &quot;https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14&quot;        )    # group images which have similar aspect ratio. In this case, we only    # group in two cases: those with width / height &gt; 1, and the other way around,    # but the code supports more general grouping strategy    aspect_grouping = [1] if cfg.DATALOADER.ASPECT_RATIO_GROUPING else []            #True    paths_catalog = import_file(        &quot;maskrcnn_benchmark.config.paths_catalog&quot;, cfg.PATHS_CATALOG, True    )    DatasetCatalog = paths_catalog.DatasetCatalog    dataset_list = cfg.DATASETS.TRAIN if is_train else cfg.DATASETS.TEST    transforms = build_transforms(cfg, is_train)    datasets = build_dataset(dataset_list, transforms, DatasetCatalog, is_train)    data_loaders = []    for dataset in datasets:        sampler = make_data_sampler(dataset, shuffle, is_distributed)        batch_sampler = make_batch_data_sampler(            dataset, sampler, aspect_grouping, images_per_gpu, num_iters, start_iter        )        collator = BatchCollator(cfg.DATALOADER.SIZE_DIVISIBILITY)        num_workers = cfg.DATALOADER.NUM_WORKERS        data_loader = torch.utils.data.DataLoader(            dataset,            num_workers=num_workers,            batch_sampler=batch_sampler,            collate_fn=collator,        )        data_loaders.append(data_loader)    if is_train:        # during training, a single (possibly concatenated) data_loader is returned        assert len(data_loaders) == 1        return data_loaders[0]    return data_loaders</code></pre><p>*<a href="#1">1.单反入门</a></p><h2 id="1">1. 单反入门</h2><p>熟练掌握摄影三要素：对焦、曝光、光圈。</p><h2 id="1.1">1.1 对焦</h2><p>对焦决定你的相片的清晰度。</p><h2 id="1.2">1.2 曝光</h2><p>不同的曝光会让照片呈现不一样的效果。</p><h2 id="1.3">1.3 光圈</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是我阅读Facebook发布基于pytorch1.0版本的mask r cnn所做的笔记。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
  </entry>
  
</feed>
